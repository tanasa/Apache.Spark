{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 14:31:31 WARN Utils: Your hostname, bogdan-ThinkPad-X1-Carbon-Gen-9 resolves to a loopback address: 127.0.1.1; using 10.0.0.242 instead (on interface wlp0s20f3)\n",
      "24/09/10 14:31:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/10 14:31:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/10 14:31:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/10 14:31:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/09/10 14:31:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Perform a simple transformation\n",
    "df_filtered = df.filter(df[\"Age\"] > 30)\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "df_filtered.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "# spark.stop()\n",
    "\n",
    "# Selecting specific columns\n",
    "df.select(\"Name\", \"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting the DataFrame\n",
    "df_sorted = df.orderBy(df[\"Age\"])\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column\n",
    "# df_with_gender = df.withColumn(\"Gender\", \"Female\")\n",
    "# df_with_gender.show()\n",
    "\n",
    "# Grouping and aggregating data\n",
    "# df_grouped = df.groupBy(\"Gender\").agg({\"Age\": \"avg\"})\n",
    "# df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV file\n",
    "# df_csv = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Writing a DataFrame to a CSV file\n",
    "# df_csv.write.csv(\"output.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "\n",
    "# Parquet is a columnar storage file format optimized for big data processing. \n",
    "# PySpark provides built-in support for reading and writing Parquet files. \n",
    "\n",
    "# Reading a Parquet file\n",
    "# df_parquet = spark.read.parquet(\"data.parquet\")\n",
    "\n",
    "# Writing a DataFrame to a Parquet file\n",
    "# df_parquet.write.parquet(\"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a JSON file\n",
    "# df_json = spark.read.json(\"data.json\")\n",
    "\n",
    "# Writing a DataFrame to a JSON file\n",
    "# df_json.write.json(\"output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 25), ('Bob', 30), ('Charlie', 35)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 14:31:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Defining a Schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), nullable=False),\n",
    "    StructField(\"Age\", IntegerType(), nullable=True),\n",
    "])\n",
    "\n",
    "# Create a DataFrame with the defined schema\n",
    "df_with_schema = spark.createDataFrame(data, schema)\n",
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Alice| 25|\n",
      "|       Bob| 30|\n",
      "|   Charlie| 35|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming a column\n",
    "df_renamed = df.withColumnRenamed(\"Name\", \"First Name\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping a column\n",
    "df_dropped = df.drop(\"Age\")\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "df_null = df.filter(df[\"Age\"].isNull())\n",
    "\n",
    "# Filling null values with a default value\n",
    "df_filled = df.fillna({\"Age\": 0})\n",
    "\n",
    "# Dropping rows with null values\n",
    "df_no_null = df.dropna()\n",
    "df_no_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing data\n",
    "df_no_missing = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing data with a default value\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "# Filling missing data with the mean value of a column\n",
    "mean_age = df.select(\"Age\").agg({\"Age\": \"mean\"}).first()[0]\n",
    "df_filled = df.fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|   Name|Age|Age_imputed|\n",
      "+-------+---+-----------+\n",
      "|  Alice| 25|         25|\n",
      "|    Bob| 30|         30|\n",
      "|Charlie| 35|         35|\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Data with Imputation\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Create an Imputer object\n",
    "imputer = Imputer(inputCols=[\"Age\"], outputCols=[\"Age_imputed\"])\n",
    "\n",
    "# Fit the imputer model on the DataFrame\n",
    "imputer_model = imputer.fit(df)\n",
    "\n",
    "# Apply the imputation on the DataFrame\n",
    "df_imputed = imputer_model.transform(df)\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and aggregating data\n",
    "\n",
    "# df_grouped = df.groupBy(\"Gender\").agg({\"Age\": \"avg\", \"Salary\": \"sum\"})\n",
    "# df_grouped.show()\n",
    "\n",
    "# Calculating descriptive statistics\n",
    "\n",
    "# df_stats = df.describe([\"Age\", \"Salary\"])\n",
    "# df_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table\n",
    "\n",
    "# df_pivot = df.groupBy(\"Gender\").pivot(\"City\").agg({\"Salary\": \"sum\"})\n",
    "# df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join :\n",
    "\n",
    "# Performing an inner join\n",
    "df_joined = df1.join(df2, df1[\"ID\"] == df2[\"ID\"], \"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join :\n",
    "\n",
    "# Performing an outer join\n",
    "df_joined = df1.join(df2, df1[\"ID\"] == df2[\"ID\"], \"outer\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union :\n",
    "\n",
    "# Combining DataFrames using union\n",
    "df_combined = df1.union(df2)\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Data Frame :\n",
    "\n",
    "# pip install kafka-python\n",
    "import kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "import kafka\n",
    "\n",
    "# Define the schema for the streaming data\n",
    "schema = StructType().add(\"name\", StringType()).add(\"age\", IntegerType())\n",
    "\n",
    "# Create a Streaming DataFrame from a Kafka topic\n",
    "streaming_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic-name\") \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the transformed streaming data to the console\n",
    "query = streaming_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# To keep the streaming job running until explicitly stopped.\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
